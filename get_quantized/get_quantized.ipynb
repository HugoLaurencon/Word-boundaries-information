{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"get_quantized.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMaAhmkJ0wPujkrSFY3I0vs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bOzSr9-SwvRv"},"source":["# Utils and installations"]},{"cell_type":"code","metadata":{"id":"lv9jLM2Pwfdx","executionInfo":{"status":"ok","timestamp":1633021063627,"user_tz":-120,"elapsed":240,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["def open_file(path_to_txt):\n","    f = open(path_to_txt, 'r')\n","    txt = [el for el in f.read().split('\\n') if el != '']\n","    f.close()\n","    return txt\n","\n","\n","def write_file(path_save, to_write):\n","    f = open(path_save, 'w')\n","    f.truncate(0)\n","    if (to_write[-1] == '\\n'):\n","        f.write(to_write[:-1])\n","    else:\n","        f.write(to_write)\n","    f.close()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Syle6QDKw05B"},"source":["!sudo apt-get install festival espeak-ng mbrola"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocWHzXH8w4KC"},"source":["!pip install phonemizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amSkjORT1Ogb"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7F5CBvQ-woto"},"source":["# Quantized for word"]},{"cell_type":"code","metadata":{"id":"a-AurwQkxFAM","executionInfo":{"status":"ok","timestamp":1633021079959,"user_tz":-120,"elapsed":218,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["def get_quantized_word(paths_to_task, type_task, path_save_quantized):\n","    \n","    if type_task not in ['similarity_task', 'score_task']:\n","        raise ValueError('type_task should be similarity_task or score_task.')\n","    \n","    task = [el for path in paths_to_task for el in open_file(path)]\n","    dic_code_sent = {}\n","    \n","    if type_task == 'similarity_task':\n","        for test in task:\n","            for word in test.split(' '):\n","                dic_code_sent[word] = word\n","                \n","    elif type_task == 'score_task':\n","        for test in task:\n","            code_1, code_2, sent_1, sent_2 = test.split(',')\n","            dic_code_sent[code_1] = sent_1\n","            dic_code_sent[code_2] = sent_2\n","            \n","    to_write = ['\\t'.join([code, dic_code_sent[code]]) for code in dic_code_sent]\n","    to_write = '\\n'.join(to_write)\n","    write_file(path_save_quantized, to_write)\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gb9brYSDxHzM"},"source":["# paths_to_task = ['/Data/tasks_quantized_scores/similarity_tasks/syn/task/syn_dev.txt',\n","#                  '/Data/tasks_quantized_scores/similarity_tasks/syn/task/syn_test.txt']\n","# type_task = 'similarity_task'\n","# path_save_quantized = '/Data/tasks_quantized_scores/similarity_tasks/syn/quantized/word/quantized.txt'\n","# get_quantized_word(paths_to_task, type_task, path_save_quantized)\n","\n","\n","# paths_to_task = ['/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_dev.txt',\n","#                  '/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_test.txt']\n","# type_task = 'score_task'\n","# path_save_quantized = '/Data/tasks_quantized_scores/score_tasks/aga_easy/quantized/word/quantized.txt'\n","# get_quantized_word(paths_to_task, type_task, path_save_quantized)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uoj5RRsizOZx"},"source":["# Quantized for phone nobound and phone bound"]},{"cell_type":"code","metadata":{"id":"gT7Yv-MVzVG5","executionInfo":{"status":"ok","timestamp":1633021082197,"user_tz":-120,"elapsed":217,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["import os\n","import pickle\n","\n","\n","def get_quantized_phone_nobound_bound(paths_to_task,\n","                                      type_task,\n","                                      path_dict_encode_phonemes,\n","                                      path_save_quantized,\n","                                      phone_nobound):\n","    \n","    if type_task not in ['similarity_task', 'score_task']:\n","        raise ValueError('type_task should be similarity_task or score_task.')\n","\n","    get_quantized_word(paths_to_task, type_task, path_save_quantized)\n","    quantized_word = open_file(path_save_quantized)\n","\n","    codes = [el.split('\\t')[0] for el in quantized_word]\n","    sents = [el.split('\\t')[1] for el in quantized_word]\n","\n","    write_file(path_save_quantized, '\\n'.join(sents))\n","\n","    os.system(f\"phonemize -b espeak -l en-us -p '-' -w ' ' '{path_save_quantized}' -o '{path_save_quantized + '.txt'}'\")\n","    os.rename(path_save_quantized + '.txt', path_save_quantized)\n","\n","    sents_phone = open_file(path_save_quantized)\n","    sents_phone = [el[:-1].split(' ') for el in sents_phone]\n","\n","    file = open(path_dict_encode_phonemes, \"rb\")\n","    dict_encode_phonemes = pickle.load(file)\n","    file.close()\n","\n","    sep = ','\n","    if not phone_nobound:\n","        sep = ',0,'\n","\n","    quantized_phone = []\n","    for i, sent_phone in enumerate(sents_phone):\n","        try:\n","            sent_phone_encoded = [','.join(list(map(dict_encode_phonemes.get, word.split('-')[:-1]))) for word in sent_phone]\n","            quantized_phone.append('\\t'.join([codes[i], sep.join(sent_phone_encoded)]))\n","        except:\n","            pass\n","\n","    write_file(path_save_quantized, '\\n'.join(quantized_phone))\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0iyDWFb-Qqd"},"source":["# paths_to_task = ['/Data/tasks_quantized_scores/similarity_tasks/syn/task/syn_dev.txt',\n","#                  '/Data/tasks_quantized_scores/similarity_tasks/syn/task/syn_test.txt']\n","# type_task = 'similarity_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_save_quantized = '/Data/tasks_quantized_scores/similarity_tasks/syn/quantized/phoneme_no_boundaries/quantized.txt'\n","# phone_nobound = True\n","# get_quantized_phone_nobound_bound(paths_to_task, type_task, path_dict_encode_phonemes, path_save_quantized, phone_nobound)\n","\n","\n","# paths_to_task = ['/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_dev.txt',\n","#                  '/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_test.txt']\n","# type_task = 'score_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_save_quantized = '/Data/tasks_quantized_scores/score_tasks/aga_easy/quantized/phoneme_no_boundaries/quantized.txt'\n","# phone_nobound = True\n","# get_quantized_phone_nobound_bound(paths_to_task, type_task, path_dict_encode_phonemes, path_save_quantized, phone_nobound)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nX_VP2EBNiJm"},"source":["# Quantized for phone nobound masking"]},{"cell_type":"code","metadata":{"id":"IQDH80brOG2u","executionInfo":{"status":"ok","timestamp":1633021085209,"user_tz":-120,"elapsed":214,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["def get_quantized_phone_nobound_masking(paths_to_task,\n","                                        type_task,\n","                                        path_dict_encode_phonemes,\n","                                        path_save_quantized):\n","  \n","    if type_task not in ['similarity_task', 'score_task']:\n","        raise ValueError('type_task should be similarity_task or score_task.')\n","  \n","    get_quantized_phone_nobound_bound(paths_to_task,\n","                                      type_task,\n","                                      path_dict_encode_phonemes,\n","                                      path_save_quantized,\n","                                      phone_nobound = False)\n","    \n","    quantized_phone_bound = open_file(path_save_quantized)\n","    quantized_phone_bound = [el.split('\\t') for el in quantized_phone_bound]\n","\n","    for i in range(len(quantized_phone_bound)):\n","        list_int_pre = list(map(int, quantized_phone_bound[i][1].split(',')))\n","\n","        list_int_post = list_int_pre * 1\n","        for k in range(1, len(list_int_post)):\n","            if (list_int_post[k-1] != 0) and (list_int_post[k] != 0):\n","                list_int_post[k] += 59\n","        list_int_post = [el for el in list_int_post if el != 0]\n","\n","        list_string_post = list(map(str, list_int_post))\n","        quantized_phone_bound[i][1] = ','.join(list_string_post)\n","\n","    quantized_phone_nobound_masking = ['\\t'.join(el) for el in quantized_phone_bound]\n","    write_file(path_save_quantized, '\\n'.join(quantized_phone_nobound_masking))\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yl9EgKN8Tr2g"},"source":["# paths_to_task = ['/Data/tasks_quantized_scores/similarity_tasks/syn/task/syn_dev.txt',\n","#                  '/Data/tasks_quantized_scores/similarity_tasks/syn/task/syn_test.txt']\n","# type_task = 'similarity_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_save_quantized = '/Data/tasks_quantized_scores/similarity_tasks/syn/quantized/phoneme_no_boundaries_masking/quantized.txt'\n","# get_quantized_phone_nobound_masking(paths_to_task, type_task, path_dict_encode_phonemes, path_save_quantized)\n","\n","\n","# paths_to_task = ['/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_dev.txt',\n","#                  '/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_test.txt']\n","# type_task = 'score_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_save_quantized = '/Data/tasks_quantized_scores/score_tasks/aga_easy/quantized/phoneme_no_boundaries_masking/quantized.txt'\n","# get_quantized_phone_nobound_masking(paths_to_task, type_task, path_dict_encode_phonemes, path_save_quantized)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vs9Luue2zSq5"},"source":["# Quantized for phone nobound masking bpe and phone nobound masking bpe nobound"]},{"cell_type":"markdown","metadata":{"id":"5E01QBe10lJX"},"source":["## Train a BPE tokenizer"]},{"cell_type":"code","metadata":{"id":"v0GLrvTR0rIY","executionInfo":{"status":"ok","timestamp":1633021088012,"user_tz":-120,"elapsed":210,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["import string\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import BpeTrainer\n","\n","\n","def encode_phoneme_bound(path_to_phoneme_bound, path_save_phonemes_encoded):\n","    \n","    f = open(path_to_phoneme_bound, 'r')\n","    phonemes = [list(map(int, el.split(' '))) for el in f.read().split('\\n') if el != '']\n","    f.close()\n","    \n","    encoding_values = string.printable[:59]\n","    dic_encoder = {i: encoding_values[i-1] for i in range(1, 60)}\n","    dic_encoder[0] = ' '\n","    \n","    phonemes_encoded = [list(map(dic_encoder.get, el)) for el in phonemes]\n","    phonemes_encoded = [''.join(el) for el in phonemes_encoded]\n","    \n","    write_file(path_save_phonemes_encoded, '\\n'.join(phonemes_encoded))\n","    \n","    return phonemes_encoded\n","\n","\n","def train_tokenizer(path_to_training_data, vocab_size, path_save_tokenizer):\n","    \n","    tokenizer = Tokenizer(BPE(unk_token = \"<unk>\"))\n","    tokenizer.pre_tokenizer = Whitespace()\n","    \n","    trainer = BpeTrainer(vocab_size = vocab_size, special_tokens = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n","    tokenizer.train(files = path_to_training_data, trainer = trainer)\n","    \n","    tokenizer.save(path_save_tokenizer)\n","    \n","\n","def encode_with_tokenizer(path_to_saved_tokenizer, path_to_phonemes_encoded, path_save_final_encoding):\n","    \n","    def split_word(word):\n","        return [char for char in word]\n","    \n","    def add_59(list_):\n","        return [el + 59 if i > 0 else el for i, el in enumerate(list_)]\n","\n","    f = open(path_to_phonemes_encoded, 'r')\n","    phonemes_encoded = f.read().split('\\n')\n","    f.close()\n","    \n","    tokenizer = Tokenizer.from_file(path_to_saved_tokenizer)\n","    \n","    encoding_values = string.printable[:59]\n","    dic_decoder = {encoding_values[i-1]: i for i in range(1, 60)}\n","    \n","    to_write = ''\n","    for sent in phonemes_encoded:\n","        output_tokenizer = tokenizer.encode(sent)\n","        decoding_values = [list(map(dic_decoder.get, split_word(word))) for word in output_tokenizer.tokens]\n","        decoding_values = [add_59(el) for el in decoding_values]\n","        decoding_values = [str(el) for sublist in decoding_values for el in sublist]\n","        to_write += ' '.join(decoding_values) + '\\n'\n","    \n","    write_file(path_save_final_encoding, to_write)\n","    "],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_74Nscs2iKs"},"source":["# path_to_phoneme_bound = '/Data/models/topline_phoneme_smallbert/with_boundaries/construction/quantized/phonemes_withboundaries_preprocessed_train_text_librispeech.txt'\n","# path_save_phonemes_encoded = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/phonemes_before_bpe_encoding_train_text_librispeech.txt'\n","# phonemes = encode_phoneme_bound(path_to_phoneme_bound, path_save_phonemes_encoded)\n","\n","\n","\n","# path_to_training_data = ['/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/phonemes_before_bpe_encoding_train_text_librispeech.txt',\n","#                          '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/phonemes_before_bpe_encoding_dev_text_librispeech.txt',\n","#                          '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/phonemes_before_bpe_encoding_test_text_librispeech.txt']\n","# vocab_size = 1000\n","# path_save_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/tokenizer_bpe_1000.json'\n","# train_tokenizer(path_to_training_data, vocab_size, path_save_tokenizer)\n","\n","\n","# path_to_training_data = ['/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/phonemes_before_bpe_encoding_train_text_librispeech.txt',\n","#                          '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/phonemes_before_bpe_encoding_dev_text_librispeech.txt',\n","#                          '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/phonemes_before_bpe_encoding_test_text_librispeech.txt']\n","# vocab_size = 1000\n","# path_save_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/tokenizer_bpe_1000.json'\n","# train_tokenizer(path_to_training_data, vocab_size, path_save_tokenizer)\n","\n","\n","\n","# path_to_saved_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/tokenizer_bpe_1000.json'\n","# path_to_phonemes_encoded = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/phonemes_before_bpe_encoding_train_text_librispeech.txt'\n","# path_save_final_encoding = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/no_boundaries_masking_bpe_1000/phonemes_noboundaries_masking_bpe_1000_preprocessed_train_text_librispeech.txt'\n","# encode_with_tokenizer(path_to_saved_tokenizer, path_to_phonemes_encoded, path_save_final_encoding)\n","\n","\n","# path_to_saved_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/tokenizer_bpe_1000.json'\n","# path_to_phonemes_encoded = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/phonemes_before_bpe_encoding_train_text_librispeech.txt'\n","# path_save_final_encoding = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/no_boundaries_masking_bpe_nobound_1000/phonemes_noboundaries_masking_bpe_nobound_1000_preprocessed_train_text_librispeech.txt'\n","# encode_with_tokenizer(path_to_saved_tokenizer, path_to_phonemes_encoded, path_save_final_encoding)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gWKYLY_gBqvM"},"source":["## Create quantized"]},{"cell_type":"code","metadata":{"id":"TQEPhzpBBsu5","executionInfo":{"status":"ok","timestamp":1633021091655,"user_tz":-120,"elapsed":216,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["def get_quantized_phone_nobound_masking_bpe(paths_to_task,\n","                                            type_task,\n","                                            path_dict_encode_phonemes,\n","                                            path_to_tokenizer,\n","                                            path_to_save_quantized,\n","                                            nobound_bpe_nobound):\n","  \n","    if type_task not in ['similarity_task', 'score_task']:\n","        raise ValueError('type_task should be similarity_task or score_task.')\n","\n","    get_quantized_phone_nobound_bound(paths_to_task,\n","                                      type_task,\n","                                      path_dict_encode_phonemes,\n","                                      path_save_quantized,\n","                                      phone_nobound = False)\n","    \n","    f = open(path_to_save_quantized, 'r')\n","    task_phoneme_bound = [el.split('\\t') for el in f.read().split('\\n') if el != '']\n","    f.close()\n","    \n","    codes = [el[0] for el in task_phoneme_bound]\n","    phoneme_bound = [' '.join(el[1].split(',')) for el in task_phoneme_bound]\n","    if nobound_bpe_nobound:\n","        phoneme_bound = [' '.join(el[1].split(',')).replace(' 0 ', ' ') for el in task_phoneme_bound]\n","    \n","    write_file(path_to_save_quantized, '\\n'.join(phoneme_bound))\n","    \n","    encode_phoneme_bound(path_to_save_quantized, path_to_save_quantized)\n","    \n","    encode_with_tokenizer(path_to_tokenizer, path_to_save_quantized, path_to_save_quantized)\n","    \n","    f = open(path_to_save_quantized, 'r')\n","    phoneme_nobound_bpe = [','.join(el.split(' ')) for el in f.read().split('\\n') if el != '']\n","    f.close()\n","    \n","    assert len(codes) == len(phoneme_nobound_bpe)\n","    \n","    quantized = [codes[i] + '\\t' + phoneme_nobound_bpe[i] for i in range(len(codes))]\n","    \n","    write_file(path_to_save_quantized, '\\n'.join(quantized))"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWm9tD5uCfv4"},"source":["# paths_to_task = ['/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_dev.txt',\n","#                  '/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_test.txt']\n","# type_task = 'score_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_to_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/tokenizer_bpe_1000.json'\n","# path_save_quantized = '/Data/tasks_quantized_scores/score_tasks/aga_easy/quantized/phoneme_no_boundaries_masking_bpe_1000/quantized.txt'\n","# nobound_bpe_nobound = False\n","# get_quantized_phone_nobound_masking_bpe(paths_to_task, type_task, path_dict_encode_phonemes, path_to_tokenizer, path_save_quantized, nobound_bpe_nobound = False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlMjuWiWKTLk"},"source":["# Quantized for phone nobound onehot bpe and phone nobound onehot bpe nobound"]},{"cell_type":"markdown","metadata":{"id":"F7bXbgjYcJiL"},"source":["## Create encoding onehot"]},{"cell_type":"code","metadata":{"id":"9sVUXh5SP5rj","executionInfo":{"status":"ok","timestamp":1633021095504,"user_tz":-120,"elapsed":225,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["import json\n","\n","\n","def create_encoding_onehot(path_to_masking_bpe_files, path_to_save_dic_encode_onehot):\n","    \n","    full_txt_bpe_masking = ''\n","    \n","    for path in path_to_masking_bpe_files:\n","        f = open(path, 'r')\n","        full_txt_bpe_masking += f.read() + '\\n'\n","        f.close()\n","    \n","    full_txt_bpe_masking = [el for el in full_txt_bpe_masking.split('\\n') if el != '']\n","\n","    bpe_to_encode = set()\n","    for i in range(1, 60):\n","        bpe_to_encode.add(str(i))\n","    \n","    for line in full_txt_bpe_masking:\n","        line_split = line.split(' ')\n","        line_split = list(map(int, line_split))\n","        \n","        ind_beg_bpe = [i for i, el in enumerate(line_split) if el <= 59]\n","        \n","        for i in range(len(ind_beg_bpe) - 1):\n","            bound = ind_beg_bpe[i + 1] - ind_beg_bpe[i]\n","            new_bpe = line_split[ind_beg_bpe[i] : ind_beg_bpe[i] + bound]\n","            new_bpe = ' '.join(list(map(str, new_bpe)))\n","            bpe_to_encode.add(new_bpe)\n","        new_bpe = line_split[ind_beg_bpe[len(ind_beg_bpe) - 1]:]\n","        new_bpe = ' '.join(list(map(str, new_bpe)))\n","        bpe_to_encode.add(new_bpe)\n","            \n","    for i in range(1, 60):\n","        bpe_to_encode.remove(str(i))\n","        \n","    bpe_to_encode = {el: i+1  for i, el in enumerate([str(i) for i in range(1, 60)] + list(bpe_to_encode))}\n","    print(\"Length of the encoding dictionary:\", len(bpe_to_encode))\n","    \n","    with open(path_to_save_dic_encode_onehot, 'w') as fp:\n","        json.dump(bpe_to_encode, fp)\n","\n","    \n","def masking_bpe_to_onehot_bpe(path_to_masking_bpe, path_to_dic_encode_onehot_bpe, path_to_save_onehot_bpe):\n","    \n","    masking_bpe = open_file(path_to_masking_bpe)\n","    \n","    with open(path_to_dic_encode_onehot_bpe) as f:\n","        dic_encode_onehot_bpe = json.load(f)\n","        \n","    onehot_bpe = ''\n","    \n","    for line in masking_bpe:\n","        line_split = line.split(' ')\n","        line_split = list(map(int, line_split))\n","        \n","        ind_beg_bpe = [i for i, el in enumerate(line_split) if el <= 59]\n","        \n","        new_line = []\n","        for i in range(len(ind_beg_bpe) - 1):\n","            bound = ind_beg_bpe[i + 1] - ind_beg_bpe[i]\n","            new_bpe = line_split[ind_beg_bpe[i] : ind_beg_bpe[i] + bound]\n","            new_bpe = ' '.join(list(map(str, new_bpe)))\n","            new_line.append(str(dic_encode_onehot_bpe[new_bpe]))\n","        new_bpe = line_split[ind_beg_bpe[len(ind_beg_bpe) - 1]:]\n","        new_bpe = ' '.join(list(map(str, new_bpe)))\n","        new_line.append(str(dic_encode_onehot_bpe[new_bpe]))\n","        onehot_bpe += ' '.join(new_line) + '\\n'\n","        \n","    write_file(path_to_save_onehot_bpe, onehot_bpe)\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6WbKvo6eNZw"},"source":["# path_to_masking_bpe_files = ['/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/no_boundaries_masking_bpe_1000/phonemes_noboundaries_masking_bpe_1000_preprocessed_train_text_librispeech.txt',\n","#                              '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/no_boundaries_masking_bpe_1000/phonemes_noboundaries_masking_bpe_1000_preprocessed_dev_text_librispeech.txt',\n","#                              '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/no_boundaries_masking_bpe_1000/phonemes_noboundaries_masking_bpe_1000_preprocessed_test_text_librispeech.txt']\n","# path_to_save_dic_encode_onehot = '/Data/models/topline_phoneme_smallbert/no_boundaries_onehot_bpe/construction/dic_encode_onehot_bpe_1000.json'\n","# create_encoding_onehot(path_to_masking_bpe_files, path_to_save_dic_encode_onehot)\n","\n","\n","# path_to_masking_bpe = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/no_boundaries_masking_bpe_1000/phonemes_noboundaries_masking_bpe_1000_preprocessed_train_text_librispeech.txt'\n","# path_to_dic_encode_onehot_bpe = '/Data/models/topline_phoneme_smallbert/no_boundaries_onehot_bpe/construction/dic_encode_onehot_bpe_1000.json'\n","# path_to_save_onehot_bpe = '/Data/models/topline_phoneme_smallbert/no_boundaries_onehot_bpe/no_boundaries_onehot_bpe_1000/construction/quantized/phonemes_noboundaries_onehot_bpe_1000_preprocessed_train_text_librispeech.txt'\n","# masking_bpe_to_onehot_bpe(path_to_masking_bpe, path_to_dic_encode_onehot_bpe, path_to_save_onehot_bpe)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUb5sKfIgngI"},"source":["## Create quantized"]},{"cell_type":"code","metadata":{"id":"UrqdGiPHgb4o","executionInfo":{"status":"ok","timestamp":1633021100081,"user_tz":-120,"elapsed":222,"user":{"displayName":"Hugo Laurençon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11375379862591739933"}}},"source":["def get_quantized_phone_nobound_onehot_bpe(paths_to_task,\n","                                           type_task,\n","                                           path_dict_encode_phonemes,\n","                                           path_to_tokenizer,\n","                                           path_to_dic_encode_onehot_bpe,\n","                                           path_to_save_quantized,\n","                                           nobound_bpe_nobound):\n","  \n","    if type_task not in ['similarity_task', 'score_task']:\n","        raise ValueError('type_task should be similarity_task or score_task.')\n","    \n","    get_quantized_phone_nobound_masking_bpe(paths_to_task,\n","                                            type_task,\n","                                            path_dict_encode_phonemes,\n","                                            path_to_tokenizer,\n","                                            path_to_save_quantized,\n","                                            nobound_bpe_nobound)\n","\n","    masking_bpe = open_file(path_to_save_quantized)\n","    masking_bpe = [el.split('\\t') for el in masking_bpe]\n","    codes_quantized = [el[0] for el in masking_bpe]\n","    masking_bpe = [el[1] for el in masking_bpe]\n","    \n","    masking_bpe = '\\n'.join(masking_bpe)\n","    masking_bpe = masking_bpe.replace(',', ' ')\n","\n","    write_file(path_to_save_quantized, masking_bpe)\n","    \n","    masking_bpe_to_onehot_bpe(path_to_save_quantized, path_to_dic_encode_onehot_bpe, path_to_save_quantized)\n","    \n","    f = open(path_to_save_quantized, 'r')\n","    onehot_bpe = f.read()\n","    f.close()\n","    \n","    onehot_bpe = onehot_bpe.replace(' ', ',')\n","    onehot_bpe = onehot_bpe.split('\\n')\n","    onehot_bpe = ['\\t'.join([codes_quantized[i], el]) for i, el in enumerate(onehot_bpe)]\n","    onehot_bpe = '\\n'.join(onehot_bpe)\n","\n","    write_file(path_to_save_quantized, onehot_bpe)\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTRjkZPpnBwd"},"source":["# paths_to_task = ['/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_dev.txt',\n","#                  '/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_test.txt']\n","# type_task = 'score_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_to_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/construction/tokenizer_bpe_1000.json'\n","# path_to_dic_encode_onehot_bpe = '/Data/models/topline_phoneme_smallbert/no_boundaries_onehot_bpe/construction/dic_encode_onehot_bpe_1000.json'\n","# path_to_save_quantized = '/Data/tasks_quantized_scores/score_tasks/aga_easy/quantized/phoneme_no_boundaries_onehot_bpe_1000/quantized.txt'\n","# nobound_bpe_nobound = False\n","# get_quantized_phone_nobound_onehot_bpe(paths_to_task, type_task, path_dict_encode_phonemes, path_to_tokenizer, path_to_dic_encode_onehot_bpe, path_to_save_quantized, nobound_bpe_nobound)\n","\n","\n","# paths_to_task = ['/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_dev.txt',\n","#                  '/Data/tasks_quantized_scores/score_tasks/aga_easy/task/aga_easy_test.txt']\n","# type_task = 'score_task'\n","# path_dict_encode_phonemes = '/Data/models/topline_phoneme_smallbert/construction/dict_encode_phonemes.pkl'\n","# path_to_tokenizer = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe_nobound/construction/tokenizer_bpe_1000.json'\n","# path_to_dic_encode_onehot_bpe = '/Data/models/topline_phoneme_smallbert/no_boundaries_onehot_bpe_nobound/construction/dic_encode_onehot_bpe_nobound_1000.json'\n","# path_to_save_quantized = '/Data/tasks_quantized_scores/score_tasks/aga_easy/quantized/phoneme_no_boundaries_onehot_bpe_nobound_1000/quantized.txt'\n","# nobound_bpe_nobound = True\n","# get_quantized_phone_nobound_onehot_bpe(paths_to_task, type_task, path_dict_encode_phonemes, path_to_tokenizer, path_to_dic_encode_onehot_bpe, path_to_save_quantized, nobound_bpe_nobound)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"csd_Eknwvhsf"},"source":["# Create quantized for phone onehot dpparse and phone onehot dpparse noduration"]},{"cell_type":"markdown","metadata":{"id":"QMdTYHjJwBJD"},"source":["## Create encoding dpparse"]},{"cell_type":"code","metadata":{"id":"A1HEqC0uvjDg"},"source":["def create_dic_encode_phonemes(path_to_segmentation, path_to_save_dic_encode):\n","    \n","    segm = open_file(path_to_segmentation)\n","    segm = [el.split(' ') for el in segm]\n","    segm = [el_ for el in segm for el_ in el]\n","    segm = set(segm)\n","    \n","    dic_encode = {el: str(i+1) for i, el in enumerate(segm)}\n","    print(\"Length of the encoding dictionary:\", len(dic_encode))\n","    \n","    with open(path_to_save_dic_encode, 'w') as fp:\n","        json.dump(dic_encode, fp)\n","        \n","\n","def create_quantized_libri(path_to_segmentation, path_to_dic_encode, path_to_save_encode_segmentation):\n","    \n","    with open(path_to_dic_encode) as f:\n","        dic_encode = json.load(f)\n","        \n","    segm = open_file(path_to_segmentation)\n","    segm = [el.split(' ') for el in segm]\n","    segm = [list(map(dic_encode.get, el)) for el in segm]\n","    segm = [' '.join(el) for el in segm]\n","    segm = '\\n'.join(segm)\n","    \n","    write_file(path_to_save_encode_segmentation, segm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKcZOHIswxws"},"source":["# path_to_segmentation = '/Data/models/topline_phoneme_smallbert/dpparse/construction/segmentation_dpparse.txt'\n","# path_to_save_dic_encode = '/Data/models/topline_phoneme_smallbert/dpparse/construction/dic_encode_dpparse.json'\n","# create_dic_encode_phonemes(path_to_segmentation, path_to_save_dic_encode)\n","\n","# path_to_segmentation = '/Data/models/topline_phoneme_smallbert/dpparse_noduration/construction/segmentation_dpparse_noduration.txt'\n","# path_to_save_dic_encode = '/Data/models/topline_phoneme_smallbert/dpparse_noduration/construction/dic_encode_dpparse_noduration.json'\n","# create_dic_encode_phonemes(path_to_segmentation, path_to_save_dic_encode)\n","\n","\n","# path_to_segmentation = '/Data/models/topline_phoneme_smallbert/dpparse/construction/segmentation_dpparse.txt'\n","# path_to_dic_encode = '/Data/models/topline_phoneme_smallbert/dpparse/construction/dic_encode_dpparse.json'\n","# path_to_save_encode_segmentation = '/Data/models/topline_phoneme_smallbert/dpparse/construction/quantized/quantized_libri_dpparse.txt'\n","# create_quantized_libri(path_to_segmentation, path_to_dic_encode, path_to_save_encode_segmentation)\n","\n","# path_to_segmentation = '/Data/models/topline_phoneme_smallbert/dpparse_noduration/construction/segmentation_dpparse_noduration.txt'\n","# path_to_dic_encode = '/Data/models/topline_phoneme_smallbert/dpparse_noduration/construction/dic_encode_dpparse_noduration.json'\n","# path_to_save_encode_segmentation = '/Data/models/topline_phoneme_smallbert/dpparse_noduration/construction/quantized/quantized_libri_dpparse_noduration.txt'\n","# create_quantized_libri(path_to_segmentation, path_to_dic_encode, path_to_save_encode_segmentation)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Whya43AV4eRe"},"source":["## Create quantized"]},{"cell_type":"code","metadata":{"id":"6YJO9w5vx3bP"},"source":["def create_quantized_task(path_to_normalseg_task, path_to_dic_encode, path_to_save_quantized):\n","    \n","    with open(path_to_dic_encode) as f:\n","        dic_encode = json.load(f)\n","        \n","    normalseg = open_file(path_to_normalseg_task)\n","    \n","    codes = [el.split(' ')[0] for el in normalseg]\n","    seg = [el.split(' ')[1:] for el in normalseg]\n","    \n","    quantized = [codes[i] + '\\t' + ','.join(list(map(dic_encode.get, el))) for i, el in enumerate(seg) if all(unit in dic_encode for unit in el)]\n","    quantized = '\\n'.join(quantized)\n","    \n","    write_file(path_to_save_quantized, quantized)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYF8VSh_yWIi"},"source":["# path_to_normalseg_task = '/normalseg_tasks_dpparse.txt'\n","# path_to_dic_encode = '/Data/models/topline_phoneme_smallbert/dpparse/construction/dic_encode_dpparse.json'\n","# path_to_save_quantized = '/quantized_tasks_dpparse.txt'\n","# create_quantized_task(path_to_normalseg_task, path_to_dic_encode, path_to_save_quantized)\n","\n","# path_to_normalseg_task = '/normalseg_tasks_dpparse_noduration.txt'\n","# path_to_dic_encode = '/Data/models/topline_phoneme_smallbert/dpparse_noduration/construction/dic_encode_dpparse_noduration.json'\n","# path_to_save_quantized = '/quantized_tasks_dpparse_noduration.txt'\n","# create_quantized_task(path_to_normalseg_task, path_to_dic_encode, path_to_save_quantized)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8y45cHBs4rPw"},"source":["# BPE to normal segmentation"]},{"cell_type":"code","metadata":{"id":"Z5kb8MhW5_xG"},"source":["def bpe_to_normal_segmentation(path_to_bpe, path_save_normal_representation, for_task = False):\n","    \n","    f = open(path_to_bpe, 'r')\n","    txt_bpe = f.read()\n","    f.close()\n","    \n","    txt_bpe = [el for el in txt_bpe.split('\\n') if el != '']\n","    \n","    if for_task: # we give the quantized for a task for the bpe model as input\n","        txt_bpe = [el.split('\\t')[1].replace(',', ' ') for el in txt_bpe]\n","    \n","    normal_repres = []\n","    \n","    for line in txt_bpe:\n","        line_split = line.split(' ')\n","        line_split = list(map(int, line_split))\n","        \n","        ind_beg_bpe = [i for i, el in enumerate(line_split) if el <= 59]\n","        \n","        normal_line = []\n","        \n","        for i in range(len(ind_beg_bpe) - 1):\n","            bound = ind_beg_bpe[i + 1] - ind_beg_bpe[i]\n","            new_bpe = line_split[ind_beg_bpe[i] : ind_beg_bpe[i] + bound]\n","            new_bpe = [el if el <= 59 else el-59 for el in new_bpe]\n","            new_bpe = ','.join(list(map(str, new_bpe)))\n","            normal_line.append(new_bpe)\n","        new_bpe = line_split[ind_beg_bpe[len(ind_beg_bpe) - 1]:]\n","        new_bpe = [el if el <= 59 else el-59 for el in new_bpe]\n","        new_bpe = ','.join(list(map(str, new_bpe)))\n","        normal_line.append(new_bpe)\n","        \n","        normal_line = ' '.join(normal_line)\n","        normal_repres.append(normal_line)\n","        \n","    normal_repres = '\\n'.join(normal_repres)\n","    \n","    f = open(path_save_normal_representation, 'w')\n","    f.truncate(0)\n","    f.write(normal_repres)\n","    f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHI7xVTX6F_E"},"source":["# path_to_bpe = '/Data/models/topline_phoneme_smallbert/no_boundaries_masking_bpe/no_boundaries_masking_bpe_1000/construction/quantized/phonemes_noboundaries_masking_bpe_1000_preprocessed_train_text_librispeech.txt'\n","# path_save_normal_representation = '/normalseg_phonemes_noboundaries_masking_bpe_1000_preprocessed_train_text_librispeech.txt'\n","# for_task = False\n","# bpe_to_normal_segmentation(path_to_bpe, path_save_normal_representation, for_task)"],"execution_count":null,"outputs":[]}]}